{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0Etzmmbd8Ee"
   },
   "outputs": [],
   "source": [
    "from builtins import range, input\n",
    "import os, sys\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrluNo7vt4p6"
   },
   "outputs": [],
   "source": [
    "lines = pd.read_csv(\"Hindi_English_Corpus.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVXe61L4uQ6y"
   },
   "outputs": [],
   "source": [
    "lines=lines[lines['source']=='ted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lt4zC5b88VM4"
   },
   "outputs": [],
   "source": [
    "engSentences = lines['english_sentence']\n",
    "hinSentences = lines['hindi_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtsJardouYag"
   },
   "outputs": [],
   "source": [
    "engSentences = engSentences[:5000]\n",
    "hinSentences = hinSentences[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1631448050019,
     "user": {
      "displayName": "Vivek Parmar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiQvSN1U7ajex8Lg2-3ghFFG69J1XGJPvy5DaAQLQ=s64",
      "userId": "16604758865318092560"
     },
     "user_tz": -330
    },
    "id": "veCPoApj_Yna",
    "outputId": "bedbfd27-2446-46f9-828e-973829bcb1f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4500,), (500,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = engSentences, hinSentences\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1,random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1631448050741,
     "user": {
      "displayName": "Vivek Parmar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiQvSN1U7ajex8Lg2-3ghFFG69J1XGJPvy5DaAQLQ=s64",
      "userId": "16604758865318092560"
     },
     "user_tz": -330
    },
    "id": "McdmYpEv-BvJ",
    "outputId": "9a11dcd7-3674-4c10-d59d-8f437e586ec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample train size: 4500\n"
     ]
    }
   ],
   "source": [
    "#For training Data\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "target_texts_inputs = []\n",
    "\n",
    "#Converting to lowercase\n",
    "en_train = [line.lower() for line in X_train]\n",
    "hin_train = [line.lower() for line in y_train]\n",
    "\n",
    "NUM_SAMPLES = len(en_train)\n",
    "print(\"Sample train size:\",NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NvpYekh-XP3"
   },
   "outputs": [],
   "source": [
    "for lines in hin_train:\n",
    "    target_texts_inputs.append('<sos>'+\" \"+ lines)\n",
    "    \n",
    "for lines in hin_train:\n",
    "    target_texts.append(lines+ \" \" +'<eos>')\n",
    "    \n",
    "for lines in en_train:\n",
    "    input_texts.append(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c920Yyu_-rRd"
   },
   "outputs": [],
   "source": [
    "tokenizer_inputs = Tokenizer()\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
    "\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "max_len_input = max(len(s) for s in input_sequences)\n",
    "\n",
    "tokenizer_outputs = Tokenizer(filters='')\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) \n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n",
    "\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "\n",
    "max_len_target = max(len(s) for s in target_sequences)\n",
    "\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Kqs0KBN_2z9"
   },
   "outputs": [],
   "source": [
    "#For Testing Data\n",
    "input_texts_test = []\n",
    "target_texts_test = [] \n",
    "target_texts_inputs_test = []\n",
    "\n",
    "en_test = [line.lower() for line in X_test]\n",
    "hin_test = [line.lower() for line in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tTo_Ln9AI5G"
   },
   "outputs": [],
   "source": [
    "for lines in hin_test:\n",
    "    target_texts_inputs_test.append('<sos>'+\" \"+ lines)\n",
    "    \n",
    "for lines in hin_test:\n",
    "    target_texts_test.append(lines+ \" \" +'<eos>')\n",
    "    \n",
    "for lines in en_test:\n",
    "    input_texts_test.append(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lL8-DoWAXz1"
   },
   "outputs": [],
   "source": [
    "input_sequences_test = tokenizer_inputs.texts_to_sequences(input_texts_test)\n",
    "\n",
    "target_sequences_test = tokenizer_outputs.texts_to_sequences(target_texts_test)\n",
    "target_sequences_inputs_test = tokenizer_outputs.texts_to_sequences(target_texts_inputs_test)\n",
    "\n",
    "encoder_inputs_test = pad_sequences(input_sequences_test, maxlen=max_len_input)\n",
    "decoder_inputs_test = pad_sequences(target_sequences_inputs_test, maxlen=max_len_target, padding='post')\n",
    "decoder_targets_test = pad_sequences(target_sequences_test, maxlen=max_len_target, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tonvKTvgAxZz"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCHS = 50\n",
    "LATENT_DIM = 256\n",
    "LATENT_DIM_DECODER = 256 \n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQo-nOc1ieAB"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join(\"/content/drive/MyDrive/NMT_Data/gloveData.txt\".format(EMBEDDING_DIM)), encoding=\"utf8\") as f:\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFs0rTg9jJpd"
   },
   "outputs": [],
   "source": [
    "num_words = len(word2idx_inputs) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "  if i < num_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnCY0TbOkmni"
   },
   "outputs": [],
   "source": [
    "def softmax_over_time(x):\n",
    "  assert(K.ndim(x) > 2)\n",
    "  e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
    "  s = K.sum(e, axis=1, keepdims=True)\n",
    "  return e / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pP-TBXJrlCqY"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=max_len_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFeha08hBZ12"
   },
   "outputs": [],
   "source": [
    "decoder_targets_one_hot = np.zeros(\n",
    "  (\n",
    "    len(input_texts),\n",
    "    max_len_target,\n",
    "    num_words_output\n",
    "  ),\n",
    "  dtype='float32'\n",
    ")\n",
    "\n",
    "for i, d in enumerate(decoder_targets):\n",
    "  for t, word in enumerate(d):\n",
    "    decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIO8OTpxtxbZ"
   },
   "outputs": [],
   "source": [
    "#encoder\n",
    "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = Bidirectional(LSTM(\n",
    "  LATENT_DIM,\n",
    "  return_sequences=True, dropout=0.2\n",
    "))\n",
    "encoder_outputs = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayXQNvdLuaGw"
   },
   "outputs": [],
   "source": [
    "#Decoder\n",
    "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
    "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Sv-flyOuozZ"
   },
   "outputs": [],
   "source": [
    "attn_repeat_layer = RepeatVector(max_len_input)\n",
    "attn_concat_layer = Concatenate(axis=-1)\n",
    "attn_dense1 = Dense(10, activation='tanh')\n",
    "attn_dense2 = Dense(1, activation=softmax_over_time)\n",
    "attn_dot = Dot(axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYV4eFhgu8gj"
   },
   "outputs": [],
   "source": [
    "def one_step_attention(h, st_1):\n",
    "  st_1 = attn_repeat_layer(st_1)\n",
    "  x = attn_concat_layer([h, st_1])\n",
    "  x = attn_dense1(x)\n",
    "  alphas = attn_dense2(x)\n",
    "  context = attn_dot([alphas, h])\n",
    "  return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8HoFG7RvLeY"
   },
   "outputs": [],
   "source": [
    "decoder_lstm = LSTM(LATENT_DIM_DECODER, return_state=True)\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "\n",
    "initial_s = Input(shape=(LATENT_DIM_DECODER,), name='s0')\n",
    "initial_c = Input(shape=(LATENT_DIM_DECODER,), name='c0')\n",
    "context_last_word_concat_layer = Concatenate(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fePkH9GWvO4W"
   },
   "outputs": [],
   "source": [
    "# s, c will be re-assigned in each iteration of the loop\n",
    "s = initial_s\n",
    "c = initial_c\n",
    "\n",
    "# collect outputs in a list at first\n",
    "outputs = []\n",
    "for t in range(max_len_target): # Ty times\n",
    "  # get the context using attention\n",
    "  context = one_step_attention(encoder_outputs, s)\n",
    "\n",
    "  # we need a different layer for each time step\n",
    "  selector = Lambda(lambda x: x[:, t:t+1])\n",
    "  xt = selector(decoder_inputs_x)\n",
    "  \n",
    "  # combine \n",
    "  decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
    "\n",
    "  # pass the combined [context, last word] into the LSTM\n",
    "  # along with [s, c]\n",
    "  # get the new [s, c] and output\n",
    "  o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n",
    "\n",
    "  # final dense layer to get next word prediction\n",
    "  decoder_outputs = decoder_dense(o)\n",
    "  outputs.append(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSKrug9NvYvS"
   },
   "outputs": [],
   "source": [
    "def stack_and_transpose(x):\n",
    "  # x is a list of length T, each element is a batch_size x output_vocab_size tensor\n",
    "  x = K.stack(x) # is now T x batch_size x output_vocab_size tensor\n",
    "  x = K.permute_dimensions(x, pattern=(1, 0, 2)) # is now batch_size x T x output_vocab_size\n",
    "  return x\n",
    "\n",
    "# make it a layerx``\n",
    "stacker = Lambda(stack_and_transpose)\n",
    "outputs = stacker(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cv7Xf2cgzm7l"
   },
   "outputs": [],
   "source": [
    "model = Model(\n",
    "  inputs=[\n",
    "    encoder_inputs_placeholder,\n",
    "    decoder_inputs_placeholder,\n",
    "    initial_s, \n",
    "    initial_c,\n",
    "  ],\n",
    "  outputs=outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6MDN9kdzrB6"
   },
   "outputs": [],
   "source": [
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqiHL2UQzuY5"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate) ,loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4148545,
     "status": "ok",
     "timestamp": 1631452255042,
     "user": {
      "displayName": "Vivek Parmar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiQvSN1U7ajex8Lg2-3ghFFG69J1XGJPvy5DaAQLQ=s64",
      "userId": "16604758865318092560"
     },
     "user_tz": -330
    },
    "id": "66yUxPZlzz55",
    "outputId": "d4cbc6f2-b3e4-49c9-d575-6868de1ee4ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 145s 12s/step - loss: 7.9547 - accuracy: 0.5710 - val_loss: 5.3906 - val_accuracy: 0.6724\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 4.1493 - accuracy: 0.6656 - val_loss: 2.9084 - val_accuracy: 0.6724\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.8717 - accuracy: 0.6658 - val_loss: 2.8875 - val_accuracy: 0.6724\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.9273 - accuracy: 0.6658 - val_loss: 2.8693 - val_accuracy: 0.6724\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.8482 - accuracy: 0.6658 - val_loss: 2.8368 - val_accuracy: 0.6724\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.7933 - accuracy: 0.6658 - val_loss: 2.7876 - val_accuracy: 0.6724\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.7346 - accuracy: 0.6658 - val_loss: 2.7365 - val_accuracy: 0.6724\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.6800 - accuracy: 0.6658 - val_loss: 2.6913 - val_accuracy: 0.6724\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.6269 - accuracy: 0.6658 - val_loss: 2.6417 - val_accuracy: 0.6724\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.5737 - accuracy: 0.6658 - val_loss: 2.5986 - val_accuracy: 0.6724\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.5313 - accuracy: 0.6658 - val_loss: 2.5729 - val_accuracy: 0.6724\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.5004 - accuracy: 0.6658 - val_loss: 2.5190 - val_accuracy: 0.6724\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.4604 - accuracy: 0.6658 - val_loss: 2.4929 - val_accuracy: 0.6724\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.4294 - accuracy: 0.6658 - val_loss: 2.4763 - val_accuracy: 0.6724\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.4117 - accuracy: 0.6658 - val_loss: 2.4805 - val_accuracy: 0.6724\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.3989 - accuracy: 0.6658 - val_loss: 2.4595 - val_accuracy: 0.6724\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.3815 - accuracy: 0.6658 - val_loss: 2.4569 - val_accuracy: 0.6724\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.3697 - accuracy: 0.6658 - val_loss: 2.4852 - val_accuracy: 0.6724\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.3839 - accuracy: 0.6658 - val_loss: 2.4395 - val_accuracy: 0.6724\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.3641 - accuracy: 0.6658 - val_loss: 2.4391 - val_accuracy: 0.6724\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.3396 - accuracy: 0.6658 - val_loss: 2.4306 - val_accuracy: 0.6724\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.3316 - accuracy: 0.6658 - val_loss: 2.4435 - val_accuracy: 0.6724\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.3240 - accuracy: 0.6678 - val_loss: 2.4562 - val_accuracy: 0.6771\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.3182 - accuracy: 0.6686 - val_loss: 2.4216 - val_accuracy: 0.6764\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.3091 - accuracy: 0.6707 - val_loss: 2.4183 - val_accuracy: 0.6773\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.2880 - accuracy: 0.6712 - val_loss: 2.4277 - val_accuracy: 0.6773\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.2798 - accuracy: 0.6711 - val_loss: 2.4312 - val_accuracy: 0.6773\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.2787 - accuracy: 0.6712 - val_loss: 2.4429 - val_accuracy: 0.6776\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.2759 - accuracy: 0.6715 - val_loss: 2.4176 - val_accuracy: 0.6779\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.2526 - accuracy: 0.6717 - val_loss: 2.4225 - val_accuracy: 0.6778\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.2448 - accuracy: 0.6716 - val_loss: 2.4450 - val_accuracy: 0.6777\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.2483 - accuracy: 0.6718 - val_loss: 2.4204 - val_accuracy: 0.6782\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.2402 - accuracy: 0.6727 - val_loss: 2.4578 - val_accuracy: 0.6784\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.2479 - accuracy: 0.6723 - val_loss: 2.4469 - val_accuracy: 0.6781\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.2386 - accuracy: 0.6722 - val_loss: 2.4223 - val_accuracy: 0.6799\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.2187 - accuracy: 0.6748 - val_loss: 2.4355 - val_accuracy: 0.6810\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.2109 - accuracy: 0.6743 - val_loss: 2.4285 - val_accuracy: 0.6785\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.1967 - accuracy: 0.6743 - val_loss: 2.4290 - val_accuracy: 0.6781\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.1910 - accuracy: 0.6768 - val_loss: 2.4310 - val_accuracy: 0.6790\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.1820 - accuracy: 0.6759 - val_loss: 2.4480 - val_accuracy: 0.6804\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.1871 - accuracy: 0.6776 - val_loss: 2.4309 - val_accuracy: 0.6770\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.1777 - accuracy: 0.6775 - val_loss: 2.4406 - val_accuracy: 0.6721\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.1784 - accuracy: 0.6796 - val_loss: 2.4352 - val_accuracy: 0.6769\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.1646 - accuracy: 0.6784 - val_loss: 2.4348 - val_accuracy: 0.6785\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.1564 - accuracy: 0.6820 - val_loss: 2.4449 - val_accuracy: 0.6742\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.1567 - accuracy: 0.6834 - val_loss: 2.4443 - val_accuracy: 0.6697\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.1486 - accuracy: 0.6831 - val_loss: 2.4625 - val_accuracy: 0.6800\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.1551 - accuracy: 0.6830 - val_loss: 2.4455 - val_accuracy: 0.6706\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 81s 10s/step - loss: 2.1440 - accuracy: 0.6835 - val_loss: 2.4568 - val_accuracy: 0.6804\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 82s 10s/step - loss: 2.1411 - accuracy: 0.6869 - val_loss: 2.4541 - val_accuracy: 0.6656\n"
     ]
    }
   ],
   "source": [
    "z = np.zeros((encoder_inputs.shape[0], LATENT_DIM_DECODER)) # initial [s, c]\n",
    "r = model.fit(\n",
    "  [encoder_inputs, decoder_inputs, z, z], decoder_targets_one_hot,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m98o-kDMONX3"
   },
   "outputs": [],
   "source": [
    "model.save(\"NMT_EngToHin_gloVe.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AsAjXmbSI7Z"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#Modifying the model for Predictions\n",
    "encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n",
    "\n",
    "# next we define a T=1 decoder model\n",
    "encoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "# no need to loop over attention steps this time because there is only one step\n",
    "context = one_step_attention(encoder_outputs_as_input, initial_s)\n",
    "\n",
    "# combine context with last word\n",
    "decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n",
    "\n",
    "# lstm and final dense\n",
    "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
    "decoder_outputs = decoder_dense(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUR8QHyQSRLU"
   },
   "outputs": [],
   "source": [
    "# create the model object\n",
    "decoder_model = Model(\n",
    "  inputs=[\n",
    "    decoder_inputs_single,\n",
    "    encoder_outputs_as_input,\n",
    "    initial_s, \n",
    "    initial_c\n",
    "  ],\n",
    "  outputs=[decoder_outputs, s, c]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2498,
     "status": "ok",
     "timestamp": 1631452305014,
     "user": {
      "displayName": "Vivek Parmar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiQvSN1U7ajex8Lg2-3ghFFG69J1XGJPvy5DaAQLQ=s64",
      "userId": "16604758865318092560"
     },
     "user_tz": -330
    },
    "id": "VNV-LhuLST_D",
    "outputId": "871d26cd-63c8-46de-8a81-9d5a3b3aca9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "decoder_model.save(\"enghin_decoder_model.h5\")\n",
    "encoder_model.save(\"enghin_encoder_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2B2Lx8ESYoX"
   },
   "outputs": [],
   "source": [
    "dx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tECXSR_jSea3"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  # Encode the input as state vectors.\n",
    "  enc_out = encoder_model.predict(input_seq)\n",
    "\n",
    "  # Generate empty target sequence of length 1.\n",
    "  target_seq = np.zeros((1, 1))\n",
    "  \n",
    "  # Populate the first character of target sequence with the start character.\n",
    "  # NOTE: tokenizer lower-cases all words\n",
    "  target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "\n",
    "  # if we get this we break\n",
    "  eos = word2idx_outputs['<eos>']\n",
    "\n",
    "\n",
    "  # [s, c] will be updated in each loop iteration\n",
    "  s = np.zeros((1, LATENT_DIM_DECODER))\n",
    "  c = np.zeros((1, LATENT_DIM_DECODER))\n",
    "\n",
    "\n",
    "  # Create the translation\n",
    "  output_sentence = []\n",
    "  for _ in range(max_len_target):\n",
    "    o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n",
    "        \n",
    "\n",
    "    # Get next word\n",
    "    idx = np.argmax(o.flatten())\n",
    "\n",
    "    # End sentence of EOS\n",
    "    if eos == idx:\n",
    "      break\n",
    "\n",
    "    word = ''\n",
    "    if idx > 0:\n",
    "      word = idx2word_trans[idx]\n",
    "      output_sentence.append(word)\n",
    "\n",
    "    # Update the decoder input\n",
    "    # which is just the word just generated\n",
    "    target_seq[0, 0] = idx\n",
    "\n",
    "  return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KzD9y4t1Smu3"
   },
   "outputs": [],
   "source": [
    "test_actual_sentence=[]\n",
    "test_predicted_sentence=[]\n",
    "for i in range(len(en_test)):\n",
    "  \n",
    "  input_seq = encoder_inputs_test[i:i+1]\n",
    "  translation = decode_sequence(input_seq)\n",
    "\n",
    "  test_actual_sentence.append(target_texts_test[i])\n",
    "  test_predicted_sentence.append(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "C-rSAM6bSpZC",
    "outputId": "fef9eeb2-1aa9-4a15-9a54-40c754e7a2d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: because if they said it, it might destroy the spell\n",
      "Predicted translation: और और और के के के के के\n",
      "Actual translation: क्योंकि अगर उन्होंने बता दिया तो उसका सारा रहस्य खुल जायेगा <eos>\n",
      "-\n",
      "Input sentence: of the germ theory of disease.\n",
      "Predicted translation: और और के के के के\n",
      "Actual translation: रोग के रोगाणु सिद्धांत की | <eos>\n",
      "-\n",
      "Input sentence: the third big thing i believe that has changed india\n",
      "Predicted translation: और और और के के के के के के के\n",
      "Actual translation: तीसरी बड़ी बात, मुझे विश्वास है, जिसने भारत को बदल दिया है <eos>\n",
      "-\n",
      "Input sentence: these are the nomad girls\n",
      "Predicted translation: और और के के\n",
      "Actual translation: ये घुमक्कड़ लड़कियां हैं <eos>\n",
      "-\n",
      "Input sentence: dr. v: supposing i'm able to produce eye care,\n",
      "Predicted translation: और और के के के के\n",
      "Actual translation: डॉ. वी.: मान लीजिये मैं आँखों के इलाज का एक अदद तरीका निकाल लूँ, <eos>\n"
     ]
    }
   ],
   "source": [
    "for i in np.random.randint(0,100,5):\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts_test[i])\n",
    "    print('Predicted translation:', test_predicted_sentence[i])\n",
    "    print('Actual translation:', target_texts_test[i])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NMT_EngToHin.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
